{
    "contents" : "\naccuracyfunc <- function(dat, lab, alpha, beta) {\n  # compute accuracy on dat set\n  # dat has features, lab has labels\n  valtest<-matrix(data=0, nrow=50) # predictions for this epoch\n  for(m in 1:50) { #for each data point in data\n    #uniformly pick at random one validation point\n    dp<-sample(1:NROW(lab),1)\n    # yi = sign(t(alpha)*xi+beta)\n    xi<-dat[dp,] # the validation row\n    prediction<-sign(sum(alpha*xi)) # 1 or -1 from generated model\n    \n    # compare prediction with respective vllab entry\n    if(prediction == lab[dp]) { # good prediction\n      valtest[m]<-1\n    }\n    else { #bad prediction\n      valtest[m]<-0\n    }\n  } # dp\n  gotright<-sum(valtest) # number of good predictions\n  acc<-gotright/(NROW(valtest))\n  return(acc)\n}\n\n\nwdat<-read.csv('adult.data', header=FALSE)\nlibrary(klaR)\nlibrary(caret)\nbigx<-wdat[c(1,3,5,11,12,13)] # continuous features\n\nmycolmeans<-colMeans(bigx, na.rm = FALSE, dims = 1)\n\n#mean-center data and scale by attriibute std dev\n#for(f in 1:NCOL(bigx)) { #for every feature\n  mymean<-sapply(bigx,mean,na.rm=FALSE)\n  mysd<-sapply(bigx,sd,na.rm=FALSE)\n  myoffsets<-t(t(bigx)-mymean)\n  myscales<-t(t(myoffsets)/mysd)\n  bigx<-myscales\n#   for(row in 1:NROW(bigx)) {\n#     old<-bigx[row, f]\n#     old<-old-mycolmeans[f]\n#     old<-old/mysd\n#     bigx[row, f]<-old\n#   }\n#}\n\n\nbigy<-wdat[,15] # labels\nbigy2<-matrix() #blank matrix\n\n#------------------------------------------------------------------\n\n# for loop to replace <=50K with -1 and >50K with 1\ncounter<-1\nfor(i in bigy) {\n  #if(identical(j, i)) {\n  if(identical(\" <=50K\", i)) {\n    bigy2[counter]<- -1\n  }\n  else {\n    bigy2[counter]<- 1\n  }\n  counter<-counter+1\n}\n\ntr_indices<-createDataPartition(y=bigy2, p=.8, list=FALSE)\n#80% training\ntrdat<-bigx[tr_indices,] # training features\ntrlab<-bigy2[tr_indices] #training labels\n#20% other\notherdat<-bigx[-tr_indices,]\notherlab<-bigy2[-tr_indices]\ntest_indices<-createDataPartition(y=otherlab, p=.5, list=FALSE)\n#10% test\ntestdat<-otherdat[test_indices,] # test features \ntestlab<-otherlab[test_indices] # test labels\n#10% validation\nvaldat<-otherdat[-test_indices,] #validation features\nvallab<-otherlab[-test_indices] #validation labels\n\n#------------------------------------------------------------------\n\nNe<-50 # number epochs\nNs<-300 # number steps\n#Ns<-1\na<-0.01\nb<-50\n#exp(1) represents e\nlambda<-c(exp(1)-3, exp(1)-2, exp(1)-1, 1) # regularization weight\nlambdaaccuracies<-matrix(data=0, ncol=NROW(lambda))\n\n# the meat.\nfor(l in 1:NROW(lambda)) { # for each lambda\n  #l<-1\n  lambdacur<-lambda[l]\n  alpha<-matrix(data=0, ncol=NCOL(bigx))\n  beta<-0 # matrix of labels\n  \n  accuracies<-matrix(data=0,ncol=Ne) #on validation for each epoch\n  yplotaccuracies<-matrix(data=0, ncol=((Ne*Ns))) # for acc val 30 steps\n  xplotaccuracies<-matrix(data=0, ncol=((Ne*Ns))) # for acc val 30 steps\n  xplot<-matrix(data=0, ncol=(Ne*Ns)) # for acc val every step in a given epoch\n  yplot<-matrix(data=0, ncol=(Ne*Ns)) # for acc val every step in a given epoch\n  pac<-1\n\n  for (i in 1:Ne){ # for each epoch\n    for (j in 1:Ns){ # for each step\n    \tn<-1/(a*i + b) #c compute step length\n    \t# choose subset of training set for validation\n    \t\n    \tnum<-sample(1:NROW(trlab),1)\n    \tynum<-trlab[num] #yi from notes\n    \txnum<-trdat[num,] #xi from notes\n    \tgamma<-sum(alpha*xnum) + beta #yk(a*x+b) in notes\n    \n    \t# update rule\n    \tif(ynum*gamma >= 1) {\n    \t  # first case\n    \t  # alpha(n+1) = alpha - n(lamda*alpha)\n    \t  temp<-lambdacur*alpha\n    \t  temp2<-n*temp\n    \t  alpha<-alpha-temp2\n    \t  \n    \t  # preserve beta\n    \t  beta<-beta\n    \t}\n    \telse {\n        # otherwise case\n        # alpha(n+1) = alpha - n(lamda*alpha - yk*x)\n        first<-lambdacur*alpha\n        second<-ynum*xnum\n        temp<-first-second\n        temp2<-n*temp\n        alpha<-alpha-temp2\n        \n        # beta(n+1) = beta -n(-yk)\n        beta<-beta-n*(-ynum)\n    \t}\n  \t\n    \t# compute accuracy of current classifier on set held out on training data\n    \t# for the epoch every 30 steps\n    \tif(j%%30==0) {\n    \t  curacc<-accuracyfunc(trdat, trlab, alpha, beta)\n    \t  yplotaccuracies[i*Ns+j]<-curacc\n    \t  xplotaccuracies[i*Ns+j]<-i*Ns+j\n    \t}\n    \t\n    \tcuracc<-accuracyfunc(trdat, trlab, alpha, beta)\n    \typlot[i*Ns+j]<-curacc\n    \txplot[i*Ns+j]<-i*Ns+j\n    \t\n    } #step\n    \n    # compute accuracy on training set (to get accuracy of model)\n    # trdat has training features, trlab has training labels\n    # alpha has the features weights, beta has the bias\n    curacc<-accuracyfunc(trdat, trlab, alpha, beta)\n    accuracies[l]<-curacc\n    \n    #plot(xplot, yplot, \txlab=\"steps\", ylab=\"accuracy\", type='p')\n    \n  } # epoch\n  \n  # generate plot for plotaccuracies (30 steps per point)\n  # x: number of steps\n  # y: accuracy (0 to 1)\n  plot(xplotaccuracies,yplotaccuracies,xlab=\"steps\", ylab=\"accuracy\",type='p')\n  title(main = paste(\"lambda = \", lambda[l]))\n  \n  # code to test lambdas on validation\n  curlambdaacc<-accuracyfunc(valdat, vallab, alpha, beta)\n  lambdaaccuracies[l]<-curlambdaacc\n  \n} # lambda\n# report best lambda\n\nplot(lambda, lambdaaccuracies)\n",
    "created" : 1454647253491.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3078436103",
    "id" : "B9F6DDC8",
    "lastKnownWriteTime" : 1454915014,
    "path" : "C:/Users/harry/projects/aml/assignment2/a.R",
    "project_path" : "a.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}
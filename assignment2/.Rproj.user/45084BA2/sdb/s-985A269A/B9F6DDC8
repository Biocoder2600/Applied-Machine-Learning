{
    "contents" : "wdat<-read.csv('adult.data', header=FALSE)\nlibrary(klaR)\nlibrary(caret)\nbigx<-wdat[c(1,3,5,11,12,13)] # continuous features\nbigy<-wdat[,15] # labels\nbigy2<-matrix() #blank matrix\n\n#------------------------------------------------------------------\n\n# for loop to replace <=50K with -1 and >50K with 1\ncounter<-0\nfor(i in bigy) {\n  #if(identical(j, i)) {\n  if(identical(\" <=50K\", i)) {\n    bigy2[counter]<- -1\n  }\n  else {\n    bigy2[counter]<- 1\n  }\n  counter<-counter+1\n}\n\ntr_indices<-createDataPartition(y=bigy2, p=.8, list=FALSE)\n#80% training\ntrdat<-bigx[tr_indices,] # training features\ntrlab<-bigy2[tr_indices] #training labels\n#20% other\notherdat<-bigx[-tr_indices,]\notherlab<-bigy2[-tr_indices]\ntest_indices<-createDataPartition(y=otherlab, p=.5, list=FALSE)\n#10% test\ntestdat<-otherdat[test_indices,] # test features \ntestlab<-otherlab[test_indices] # test labels\n#10% validation\nvaldat<-otherdat[-test_indices,] #validation features\nvallab<-otherlab[-test_indices] #validation labels\n\n#------------------------------------------------------------------\n\n# variables in global scope for debugging\n\nNe<-50 # number epochs\nNs<-300 # number steps\na<-0.01\nb<-50\n#exp(1) represents e\nlambda<-c(exp(1)-3, exp(1)-2, exp(1)-1, 1) # regularization weight\nlc<-0 #lambdacounter\n\nlambdacur<-0\nalphas<-matrix(data=0, nrow=NROW(lambda), ncol=NCOL(bigx))\nbetas<-matrix(data=0, ncol=NROW(lambda))\naccuracies_matrix<-matrix(data=0, nrow=NROW(lambda), ncol=((Ne*Ns)/30)) #row per lambda, cols accuracies over time\nyplotaccuracies_matrix<-matrix(data=0, nrow=NROW(lambda), ncol=((Ne*Ns)/30)) #row per lambda, cols accuracies over time\nxplotaccuracies_matrix<-matrix(data=0, nrow=NROW(lambda), ncol=((Ne*Ns)/30)) #row per lambda, cols accuracies over time\nnum<-1\nynum<-trlab[num]\nxnum<-trdat[num,]\nalpha<-matrix(data=0, ncol=NCOL(bigx))\nbeta<-0 # matrix of labels\ngamma<-sum(alpha*xnum) + beta\ntemp<-0\ntemp2<-0\nfirst<-0\nsecond<-0\ncuracc<-0\n\n# the meat.\nfor(l in 1:NROW(lambda)) { # for each lambda\n  lambdacur<-lambda[l]\n  alpha<-matrix(data=0, ncol=NCOL(bigx))\n  beta<-0 # matrix of labels\n  \n  accuracies<-matrix(data=0,ncol=Ne) #on validation for each epoch\n  yplotaccuracies<-matrix(data=0, ncol=((Ne*Ns)/30)) # for acc val 30 steps\n  xplotaccuracies<-matrix(data=0, ncol=((Ne*Ns)/30)) # for acc val 30 steps\n  pac<-1\n  \n  # choose random starting point\n  #a0<-0 # choose random set of attributes\n  #b0<-0 # choose label corresponding to that random set?\n  #u0<-[a0,b0] \n  \n\n  for (i in 1:Ne){ # for each epoch\n    for (j in 1:Ns){ # for each step\n    \tn<-1/(a*i + b) #c compute step length\n    \t# choose subset of training set for validation\n      \n    \t# select a single data item uniformly and at random\n    \t# choose number between 1 and NROW(trlab) or NROW(trdat)\n    \t\n    \tnum<-sample(1:NROW(trlab),1)\n    \tynum<-trlab[num] #yi from notes\n    \txnum<-trdat[num,] #xi from notes\n    \tgamma<-sum(alpha*xnum) + beta #yk(a*x+b) in notes\n    \n    \t# update rule\n    \tif(ynum*gamma >= 1) {\n    \t  # first case\n    \t  # alpha(n+1) = alpha - n(lamda*alpha)\n    \t  temp<-lambdacur*alpha\n    \t  temp2<-n*temp\n    \t  alpha<-alpha-temp2\n    \t  \n    \t  # preserve beta\n    \t  beta<-beta\n    \t}\n    \telse {\n        # otherwise case\n        # alpha(n+1) = alpha - n(lamda*alpha - yk*x)\n        first<-lambdacur*alpha\n        second<-ynum*xnum\n        temp<-first-second\n        temp2<-n*temp\n        alpha<-alpha-temp2\n        \n        # beta(n+1) = beta -n(-yk)\n        beta<-beta-n*(-ynum)\n    \t}\n  \t\n    \t# compute accuracy of current classifier on set held out\n    \t# for the epoch every 30 steps\n    \tif(j%%30==0) {\n    \t  curacc<-accuracyfunc(valdat, vallab, alpha, beta)\n    \t  yplotaccuracies[pac]<-curacc\n    \t  xplotaccuracies[pac]<-j\n    \t  pac<-pac+1\n    \t}\n    \t\n    } #step\n    \n    # compute accuracy on validation set\n    # valdat has validation features, vallab has validation labels\n    # alpha has the features weights, beta has the bias\n    curacc<-accuracyfunc(valdat, vallab, alpha, beta)\n    accuracies[l]<-curacc\n    \n  } # epoch\n  \n  # generate plot for plotaccuracies (30 steps per point)\n  # x: number of steps\n  # y: accuracy (0 to 1)\n  plot(xplotaccuracies,yplotaccuracies)\n  \n  \n  # store into global env\n  alphas[lc]<-alpha\n  betas[lc]<-beta\n  accuracies_matrix[lc,]<-accuracies\n  yplotaccuracies_matrix[lc,]<-yplotaccuracies\n  xplotaccuracies_matrix[lc,]<-xplotaccuracies\n  \n  lc<-lc+1\n} # lambda\n\n\n\naccuracyfunc <- function(valdat, vallab, alpha, beta) {\n  # compute accuracy on validation set\n  # valdat has validation features, vallab has validation labels\n  valtest<-matrix(data=0, ncol=NROW(vallab)) # predictions for this epoch\n  for(m in 1:50) { #for each data point in validation\n    #uniformly pick at random one validation point\n    dp<-sample(1:NROW(vallab),1)\n    # yi = sign(t(alpha)*xi+beta)\n    xi<-valdat[dp,] # the validation row\n    prediction<-sign(sum(alpha*xi)) # 1 or -1 from generated model\n    \n    # compare prediction with respective vllab entry\n    if(prediction == vallab[dp]) { # good prediction\n      valtest[dp]<-1\n    }\n    else { #bad prediction\n      valtest[dp]<-0\n    }\n  } # dp\n  gotright<-sum(valtest) # number of good predictions\n  acc<-gotright/(NROW(valtest))\n  return(acc)\n}\n\n",
    "created" : 1454647253491.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3239591053",
    "id" : "B9F6DDC8",
    "lastKnownWriteTime" : 1454885320,
    "path" : "C:/Users/harry/projects/aml/assignment2/a.R",
    "project_path" : "a.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}